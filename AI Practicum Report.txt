Dominic B

AI Practicum Report

1. 
The problem I addressed in this assignment was how to play a game of Tic-Tac-Toe optimally. Tic-Tac-Toe is whats known as a solved game. “A solved game is a game whose outcome (win, lose or draw) can be correctly predicted from any position, assuming that both players play perfectly.” [1]. In the case of this game that means that if you are the first person to take a turn you can either win or draw every time. I wanted to know just how to do this by training an AI to do it for me.

2.
To train an AI to be able to win or draw in any game of Tic-Tac-Toe I decided to take a reinforcement learning approach and simply reward the AI for a win and punish it for a loss. After enough games played the AI would be able to beat me or draw when given the first move. 
I first looked to see if anybody had already done this and came across an article called “Reinforcement Learning: Train a bot to play tic-tac-toe.” [3] that influenced my approach. I already had the idea for a Tic-Tac-Toe that learned based on past games but the approach was helpful. I’m not sure how much he trained his AI but from the example he shows I think my had better results.
After a little research into what I wanted my game to look like I started coding. I started by just making a simple Tic-Tac-Toe game in which the AI would choose a move at random from the given open spaces. Since Tic-Tac-Toe is a solved game, this means that assuming perfect play the player that makes the first move will either win or draw every game. With this knowledge I gave the AI the first move every game to train it to be able to win or draw every game. With an AI that will randomly choose a move it was fairly easy to win against it with basic strategy even when it was given the first move. 
Once I started to implement reinforcement learning for the AI I realized that I wanted to be able to play the game hundreds of times and use past game data so I needed a way to store the data from each game. I decided that the best way to do this would be to save it in the form of text files. The AI functions by taking its current game state at the start of its turn and comparing all of the moves it could possibly make to find the greatest odds of winning based on past games. To do this I had to save every game state that had been played along with the number of times that state lead to a win for the AI. I also saved the number of games played to later get a ratio of wins for each state to total games played. This comparison would tell the AI the chance of winning for each state. I also made it so that if there was a tie for the best decision then the AI would pick one at random. This was especially important at the beginning of the AI learning process. Since all of the odds are equal at the beginning, it's important for the AI to explore all possible options by random selection. 
Saving the data to text files was probably the most difficult part of this implementation, but it seems to work well. There are three text files that hold the data. One for game states, game state wins, and total games. The game state is stored just after the AI makes a move so that in the future it can compare that move to others it could make. Then once the game ends the program goes through all of the stored game states and checks to see if they have appeared in past games. If the state was in a past game it's already in the file of states and the number of wins is changed based on the outcome of the game, reduce wins for loss increase for win or draw, in its file at the same index as the state in its file. If  the state was not in one of the past games it's added to the end of the file of states. The final file just holds the number of games played and it is increased by one for every game played. 
Since I wanted to encourage both wins and draws I decided to weigh a win as two and a draw as one. I also wanted to highly discourage a loss, so I weighed a loss as negative two. I started by weighing a draw as zero or neutral but realized that with a goal of either a win or a draw that I should also be slightly encouraging draws. 
I think that if I had just weighed a loss as zero it would have similar effect but I assumed that by weighing a loss as negative it would help to speed up the learning process by highly discouraging losing. 
 
3.
After creating an AI that could learn I then had to train it. My intention was to just play against it as though I was playing against a real opponent and I think that is a good approach but it's also very time consuming. After playing over 100 games against it and starting over multiple times because of bugs I wanted to see if there was a better way to train it. I decided to try and see if an AI that would randomly pick moves would be able to train my AI. After 50,000 games my AI would win about 90% of games but after looking at more of the data I realized there is a big problem with randomly selecting moves. My AI would learn the basics like how to win but then wasn't challenged on that by the random AI so it would just select the same moves over and over because the random AI would rarely get in its way. This reinforced it so much that my AI had only picked the same first move and only two different moves its next turn through 50,000 games.
I decided that it was best for me to just train it myself and looked into the best way to do that. After referencing a paper called “Flexible Strategy Use in Young Children’s Tic-Tac-Toe” [2] from the cognitive science department at Carnegie Mellon University I got an idea of the win conditions in Tic-Tac-Toe. It sets out a strategy that I used to both train the AI and determine its ability. The first priority is to win. This means that if you have two in a row and it's your turn you should always move to the third since it leads to a win. The second priority is to block your opponent. This means that if it's your turn and your opponent has two in a row you should block them from winning. These are the main rules I played by to train the AI. The next two are a little more advanced and I think I somewhat used while play against the AI. They involve creating forks so that there are two possibilities for a win on your next turn and blocking your opponent from creating these forks. The last four are about which spaces to give priority to. This is where the AI started to show progress. The last four give priority to the center, opposing corners, empty corner, and empty side respectively. This means that the best move is in the middle then a corner opposite to an opponent in the corner then an empty corner then an empty side. This is what I started to notice most in the AI. After over 100 games played against the AI it started to only choose the center for the first move and would then priorities opposing corners, edges, and sides in that order. 
I think it may have been useful to take the time to create another AI that is just programmed to follow the strategy I explained. Then I would be able to just the play the two AIs against each other to train it but it would be difficult to train the new AI to recognize things like forks. 
Since there are so many possible combinations of game states I have not reached a finished AI and there are some unexplored paths but I think that this behavior gives an answer to my problem. 


[-0.06956521739130435, -0.08695652173913043, -0.06956521739130435, -0.08695652173913043, 0.23478260869565218, -0.0782608695652174, -0.08695652173913043, -0.06956521739130435, -0.08695652173913043]

This list shows what the AI is given to make its decision on which move to make first. You can see that it has concluded it gains an extreme advantage from starting in the center since it is the only choice that is even positive. You would expect that the corners would be weighed the next best but my AI realized that the center was best before it could even make any real distinction between a side and a corner. At the very least it has given solid evidence that the best first move is the center.  
As of now I have played 115 games against the current iteration of the AI, and it has stored data for 205 unique game states. Since there are three states for each cell and nine cells this means there are a total of 3^9 or 19,683 possible unique states. Though not all of these apply to this situation since we will never have an instance where X starts and at this point the AI will only ever start in the center which reduces the amount of possible states by a large fraction. 
On the next page you can see an example of how the AI has learned what to prioritise. It starts in the middle, so I pick an edge based on the priority in the strategy I explained. The AI then picks an opposing corner. I pick an open corner and then from there it is just back and forth blocking wins until the game ends in a draw. 
	
4.
I think that I had good results and found a solid answer to my problem or at least created an AI that could solve the problem. I think playing more games would have given me a more optimal AI but I think in its current state it proves that it has the potential to solve my main question of how to win a game. It also provides solid evidence that the general strategy and priorities outlined in “Flexible Strategy Use in Young Children’s Tic-Tac-Toe”[2] are good guidelines for optimal play. I can confidently conclude from just 100 games against the AI that the center is the best first move. After that first move the priority of a win, block, fork, block fork, center, opposite corner, empty corner, and empty edge is also supported by my AI. I also discovered that to train an AI like this the opponent has to be able to implement some sort of strategy. My randomized AI after 50,000 did not train an AI that would be able to compete with the one that I trained after only 100 games. 
I think that if I could get my AI to a perfect 100% win or draw rate then it could be used to train an AI faster than I was able to train it. That's not necessarily a conclusion from my results but something that I found interesting while working on this project.


References

[1] Wikipedia. 2019. Solved Game. (Feb 2019). Retrieved April 30, 2019 from  https://en.wikipedia.org/wiki/Solved_game

[2] Kevin Crowley. 1993. Flexible Strategy Use in Young Children’s Tic-Tac-Toe. (1993). Retrieved April 30, 2019 from https://onlinelibrary.wiley.com/doi/epdf/10.1207/s15516709cog1704_3

[3] Amresh Venugopal. 2018. Reinforcement Learning: Train a bot to play tic-tac-toe.. (Sep 2018). Retrieved April 30, 2019 from https://becominghuman.ai/reinforcement-learning-step-by-step-17cde7dbc56c

[4] Prateek Bajaj. 2017. Reinforcement learning. (Feb 2017). Retrieved April 30, 2019 from https://www.geeksforgeeks.org/what-is-reinforcement-learning/

[5] Mauro Comi. 2018. How to teach AI to play Games: Deep Reinforcement Learning. (Nov 2018). Retrieved April 30, 2019 from https://towardsdatascience.com/how-to-teach-an-ai-to-play-games-deep-reinforcement-learning-28f9b920440a

